{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"vamos a implementar un colorizador guidado por promt. La idea principar era obtener una clasificacion de lo que aparece en la imagen (CLIP o BLIP) y luego pasarlo como promt, pero de esta manera se pierde informacion, ya que desde los embeddings de lo que aparece a la imagen creamos un texto (discreto) para luego volver a codificarlo para pasarselo al modelo. \n\nVamos a pasar directamente al modelo via cross-attention o concatenacion el contenido de la imágen.\n\ncomplementaciones: podemos decodificar el contenido de la img en texto para ofrecer salida tupla (img_colorized, text_content):","metadata":{}},{"cell_type":"markdown","source":"### Preproceso datos\n\nusamos imagenette pq es más pequeño","metadata":{}},{"cell_type":"code","source":"# Ejecuta esto en la primera celda\n!pip install datasets transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:25:23.326851Z","iopub.execute_input":"2025-12-02T16:25:23.327159Z","iopub.status.idle":"2025-12-02T16:25:30.968500Z","shell.execute_reply.started":"2025-12-02T16:25:23.327137Z","shell.execute_reply":"2025-12-02T16:25:30.967779Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.20.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pyarrow-22.0.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom transformers import CLIPVisionModel, CLIPImageProcessor\nfrom PIL import Image\nimport numpy as np\nimport os\n\n# --- CONFIGURACIÓN ---\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nIMG_SIZE = 224\nBATCH_SIZE = 32 \nEMBED_DIM = 768\n\nprint(f\"Usando dispositivo: {DEVICE}\")\n\n# --- 1. DESCARGA MANUAL DEL DATASET (Imagenette) ---\n# Si la carpeta no existe, descargamos y descomprimimos\nif not os.path.exists('imagenette2-320'):\n    print(\"Descargando Imagenette (aprox 300MB)...\")\n    !wget -q https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\n    print(\"Descomprimiendo...\")\n    !tar -xzf imagenette2-320.tgz\n    print(\"¡Listo!\")\nelse:\n    print(\"El dataset ya está descargado.\")\n\n# --- 2. CLASE DATASET MODIFICADA ---\nclass ColorizationDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        # Usamos ImageFolder: lee automáticamente las carpetas de imágenes\n        self.dataset = ImageFolder(root=root_dir)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        # ImageFolder devuelve una tupla: (PIL Image, Class Index)\n        # Solo nos interesa la imagen 'img_rgb', ignoramos la etiqueta '_'\n        img_rgb, _ = self.dataset[idx] \n        \n        # Aseguramos que sea RGB (por si hay alguna BN colada)\n        img_rgb = img_rgb.convert('RGB')\n        \n        # 1. Transformaciones (Resize, Crop, Tensor)\n        if self.transform:\n            img_tensor = self.transform(img_rgb) # [3, 224, 224] RGB\n            \n        # 2. Crear versión escala de grises\n        # Usamos transformación estándar de RGB a Gris (1 canal)\n        gray_tensor = transforms.functional.rgb_to_grayscale(img_tensor, num_output_channels=1)\n        \n        # 3. Replicar canales para entrada (opcional, pero útil para CLIP/UNet inputs estándar)\n        # Entrada U-Net: (3, 224, 224) donde R=G=B\n        gray_3ch = gray_tensor.repeat(3, 1, 1)\n        \n        return gray_3ch, img_tensor # Input (Gris), Target (Color)\n\n# --- 3. DATALOADER ---\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(), # Añadimos un poco de variedad\n    transforms.ToTensor(), \n    transforms.Normalize((0.5,), (0.5,)) \n])\n\n# Apuntamos a la carpeta 'train' dentro de lo que descomprimimos\ntrain_dataset = ColorizationDataset(root_dir='./imagenette2-320/train', transform=transform)\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    num_workers=2, \n    pin_memory=True\n)\n\nprint(f\"Dataset cargado correctamente. Imágenes totales: {len(train_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:43:47.381358Z","iopub.execute_input":"2025-12-02T16:43:47.382261Z","iopub.status.idle":"2025-12-02T16:43:58.026951Z","shell.execute_reply.started":"2025-12-02T16:43:47.382222Z","shell.execute_reply":"2025-12-02T16:43:58.025892Z"}},"outputs":[{"name":"stdout","text":"Usando dispositivo: cuda\nDescargando Imagenette (aprox 300MB)...\nDescomprimiendo...\n¡Listo!\nDataset cargado correctamente. Imágenes totales: 9469\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class CLIPFeatureExtractor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Usamos CLIP ViT-Base\n        self.model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Congelar pesos para no gastar memoria entrenando esto\n        for param in self.model.parameters():\n            param.requires_grad = False\n            \n    def forward(self, img_tensor):\n        # CLIP espera una normalización específica, pero para este prototipo rápido\n        # usaremos el tensor tal cual viene del loader (aproximación válida para test)\n        # img_tensor shape: [B, 3, 224, 224]\n        \n        # Obtenemos features\n        outputs = self.model(pixel_values=img_tensor)\n        # pooler_output es el vector resumen [B, 768]\n        return outputs.pooler_output\n\n# Instanciamos fuera del bucle\nclip_extractor = CLIPFeatureExtractor().to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:45:14.118474Z","iopub.execute_input":"2025-12-02T16:45:14.119524Z","iopub.status.idle":"2025-12-02T16:45:18.830592Z","shell.execute_reply.started":"2025-12-02T16:45:14.119458Z","shell.execute_reply":"2025-12-02T16:45:18.829564Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ae7dd8d10e44777ae001743271e29cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25807251b7b141ec9d0219bfc825c2c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c392f570d6754f2bb28c1659a357c8ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e833fc180ce6487bb67138d241e439ab"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"crear modelo","metadata":{}},{"cell_type":"code","source":"class ConditionalUNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # --- ENCODER ---\n        # Reduce tamaño, aumenta canales\n        self.enc1 = self.conv_block(3, 64)\n        self.enc2 = self.conv_block(64, 128)\n        self.enc3 = self.conv_block(128, 256)\n        self.pool = nn.MaxPool2d(2)\n        \n        # --- BOTTLENECK + FUSIÓN ---\n        # Aquí la imagen es 28x28 (224 / 2^3)\n        self.bottleneck = self.conv_block(256, 512)\n        \n        # Proyección del Embedding de CLIP para mezclarlo\n        # Queremos concatenar el embedding a la imagen latente\n        self.embed_proj = nn.Linear(EMBED_DIM, 512) \n        \n        # --- DECODER ---\n        # Aumenta tamaño, reduce canales\n        # Input channels = 512 (de abajo) + 512 (embedding) = 1024\n        self.up3 = nn.ConvTranspose2d(1024, 256, kernel_size=2, stride=2)\n        self.dec3 = self.conv_block(512, 256) # 256 del up + 256 del skip connection\n        \n        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec2 = self.conv_block(256, 128)\n        \n        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.dec1 = self.conv_block(128, 64)\n        \n        # Salida final RGB\n        self.final = nn.Conv2d(64, 3, kernel_size=1)\n        \n    def conv_block(self, in_c, out_c):\n        return nn.Sequential(\n            nn.Conv2d(in_c, out_c, 3, padding=1),\n            nn.BatchNorm2d(out_c),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_c, out_c, 3, padding=1),\n            nn.BatchNorm2d(out_c),\n            nn.ReLU(inplace=True)\n        )\n        \n    def forward(self, x, embed):\n        # x: [Batch, 3, 224, 224] (Grayscale replicado)\n        # embed: [Batch, 768]\n        \n        # Encoder\n        e1 = self.enc1(x)\n        p1 = self.pool(e1) # 112\n        \n        e2 = self.enc2(p1)\n        p2 = self.pool(e2) # 56\n        \n        e3 = self.enc3(p2)\n        p3 = self.pool(e3) # 28\n        \n        # Bottleneck\n        b = self.bottleneck(p3) # [B, 512, 28, 28]\n        \n        # --- FUSIÓN DE EMBEDDING ---\n        # 1. Proyectar embedding: [B, 768] -> [B, 512]\n        emb_vec = self.embed_proj(embed) \n        # 2. Replicar espacialmente para que coincida con la imagen (28x28)\n        emb_map = emb_vec.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, 28, 28)\n        # 3. Concatenar: [B, 512, 28, 28] + [B, 512, 28, 28] -> [B, 1024, 28, 28]\n        fused = torch.cat([b, emb_map], dim=1)\n        \n        # Decoder\n        d3 = self.up3(fused)        # -> [B, 256, 56, 56]\n        d3 = torch.cat([d3, e3], dim=1) # Skip connection\n        d3 = self.dec3(d3)\n        \n        d2 = self.up2(d3)           # -> [B, 128, 112, 112]\n        d2 = torch.cat([d2, e2], dim=1)\n        d2 = self.dec2(d2)\n        \n        d1 = self.up1(d2)           # -> [B, 64, 224, 224]\n        d1 = torch.cat([d1, e1], dim=1)\n        d1 = self.dec1(d1)\n        \n        return torch.tanh(self.final(d1)) # Salida [-1, 1]\n\nmodel = ConditionalUNet().to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.MSELoss() # L2 Loss para colores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:47:13.231883Z","iopub.execute_input":"2025-12-02T16:47:13.232225Z","iopub.status.idle":"2025-12-02T16:47:13.321625Z","shell.execute_reply.started":"2025-12-02T16:47:13.232199Z","shell.execute_reply":"2025-12-02T16:47:13.320826Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Entrenamiento","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nEPOCHS = 30 # Pocas épocas para probar rápido (sube a 20-50 para resultados reales)\nscaler = torch.cuda.amp.GradScaler() # Para Mixed Precision\n\nprint(\"Iniciando entrenamiento...\")\n\nfor epoch in range(EPOCHS):\n    model.train()\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n    \n    for gray_imgs, color_real in loop:\n        gray_imgs = gray_imgs.to(DEVICE)\n        color_real = color_real.to(DEVICE)\n        \n        # 1. Extraer Embedding de la imagen (usamos la gris para simular inferencia real)\n        # Truco: CLIP funciona mejor con la original a color, pero en inferencia \n        # real solo tendrás la gris. Si quieres robustez, usa la gris aquí también.\n        with torch.no_grad():\n            # Nota: CLIP espera RGB. gray_imgs ya tiene 3 canales repetidos.\n            embeddings = clip_extractor(gray_imgs) \n        \n        # 2. Forward Pass (Mixed Precision)\n        with torch.cuda.amp.autocast():\n            color_pred = model(gray_imgs, embeddings)\n            loss = criterion(color_pred, color_real)\n            \n        # 3. Backward\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        loop.set_postfix(loss=loss.item())\n\n# Guardar modelo\ntorch.save(model.state_dict(), \"colorizer_clip_model.pth\")\nprint(\"¡Entrenamiento completado!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:48:31.700134Z","iopub.execute_input":"2025-12-02T16:48:31.700715Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/3390991111.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler() # Para Mixed Precision\n","output_type":"stream"},{"name":"stdout","text":"Iniciando entrenamiento...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/30:   0%|          | 0/296 [00:00<?, ?it/s]/tmp/ipykernel_47/3390991111.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 1/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0248]\nEpoch 2/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0292]\nEpoch 3/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.025] \nEpoch 4/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0191]\nEpoch 5/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0171]\nEpoch 6/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0142]\nEpoch 7/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0217]\nEpoch 8/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0295]\nEpoch 9/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0234]\nEpoch 10/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0176]\nEpoch 11/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.019] \nEpoch 12/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0196]\nEpoch 13/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0316]\nEpoch 14/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0166]\nEpoch 15/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0191]\nEpoch 16/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0235]\nEpoch 17/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0216]\nEpoch 18/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0177]\nEpoch 19/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.015] \nEpoch 20/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.015] \nEpoch 21/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0162]\nEpoch 22/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0168]\nEpoch 23/30: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s, loss=0.0198] \nEpoch 24/30:  36%|███▌      | 107/296 [01:00<01:46,  1.77it/s, loss=0.0206]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Resultados","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nmodel.eval()\n# Coger un batch de ejemplo\ngray, real = next(iter(train_loader))\ngray = gray.to(DEVICE)\n\nwith torch.no_grad():\n    emb = clip_extractor(gray)\n    fake = model(gray, emb)\n\n# Función para desnormalizar y mostrar\ndef show_tensor(tensor_img):\n    img = tensor_img.cpu().clone()\n    img = img * 0.5 + 0.5 # Deshacer normalización [-1, 1]\n    img = img.permute(1, 2, 0).clamp(0, 1) # [C,H,W] -> [H,W,C]\n    return img\n\nfig, axs = plt.subplots(3, 3, figsize=(12, 12))\nfor i in range(3):\n    axs[i, 0].imshow(show_tensor(gray[i]), cmap='gray')\n    axs[i, 0].set_title(\"Input (Gris)\")\n    axs[i, 1].imshow(show_tensor(fake[i]))\n    axs[i, 1].set_title(\"Predicción (Con CLIP)\")\n    axs[i, 2].imshow(show_tensor(real[i]))\n    axs[i, 2].set_title(\"Real (Ground Truth)\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os\n\n# Define el nombre del archivo donde se guardará el modelo\nMODEL_PATH = \"colorizer_clip_guided.pth\"\n\n# Asegúrate de que el modelo esté en modo CPU antes de guardar, aunque no es estrictamente necesario,\n# a veces previene problemas al cargar en diferentes entornos.\n# model.to('cpu') \n\n# Crear el diccionario de punto de control (checkpoint)\ncheckpoint = {\n    'epoch': epoch, # Guarda la última época entrenada\n    'model_state_dict': model.state_dict(), # Diccionario de pesos del modelo\n    'optimizer_state_dict': optimizer.state_dict(), # Diccionario de estados del optimizador (crucial para reanudar)\n    'loss': loss.item(), # Guarda el valor de la última pérdida\n    'img_size': IMG_SIZE, \n    'batch_size': BATCH_SIZE\n}\n\n# Guardar el diccionario usando torch.save()\ntorch.save(checkpoint, MODEL_PATH)\n\nprint(f\"Modelo guardado exitosamente en: {MODEL_PATH}\")\n\n# Si solo quieres guardar el modelo sin la info de entrenamiento:\n# torch.save(model.state_dict(), \"colorizer_final_weights.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}