{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39470,"sourceType":"datasetVersion","datasetId":30516}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Para los datos de entrenamiento vamos a hacer un etiquetado sintético del color de las imágenes.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tqdm.keras import TqdmCallback\nimport numpy as np\nimport os\n\n# --- 1. CONFIGURACIÓN ---\nIMG_SIZE = (96, 96)\nBATCH_SIZE = 32\nSEED = 42\n\ntrain_path = \"/kaggle/input/stl10/unlabeled_images\"\ntest_path = \"/kaggle/input/stl10/test_images\"\n\n# --- 2. LÓGICA DE PROMPTS SINTÉTICOS ---\n# Paleta para detectar el color dominante\nCOLOR_PALETTE = {\n    \"red\":   np.array([1.0, 0.0, 0.0]),\n    \"green\": np.array([0.0, 1.0, 0.0]),\n    \"blue\":  np.array([0.0, 0.0, 1.0]),\n    \"yellow\":np.array([1.0, 1.0, 0.0]),\n    \"cyan\":  np.array([0.0, 1.0, 1.0]),\n    \"magenta\":np.array([1.0, 0.0, 1.0]),\n    \"white\": np.array([1.0, 1.0, 1.0]),\n    \"black\": np.array([0.0, 0.0, 0.0]),\n    \"gray\":  np.array([0.5, 0.5, 0.5]),\n    \"orange\":np.array([1.0, 0.5, 0.0]),\n    \"purple\":np.array([0.5, 0.0, 0.5]),\n    \"brown\": np.array([0.6, 0.4, 0.2])\n}\npalette_keys = list(COLOR_PALETTE.keys())\npalette_vals = np.array(list(COLOR_PALETTE.values()), dtype=np.float32)\n\ndef generate_prompt(img_rgb):\n    \"\"\"Calcula color medio y devuelve string (ej: 'color red')\"\"\"\n    mean_color = np.mean(img_rgb, axis=(0, 1)) \n    distances = np.linalg.norm(palette_vals - mean_color, axis=1)\n    closest_index = np.argmin(distances)\n    color_name = palette_keys[closest_index]\n    return \"color \" + color_name\n\n# --- 3. PIPELINE DE DATOS ---\ndef process_image(file_path):\n    # Cargar y decodificar\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_png(img, channels=3)\n    img = tf.image.resize(img, IMG_SIZE)\n    img = tf.cast(img, tf.float32) / 255.0\n    \n    # Input 1: Escala de grises\n    gray = tf.image.rgb_to_grayscale(img)\n    \n    # Input 2: Prompt de texto (generado al vuelo)\n    prompt = tf.py_function(func=generate_prompt, inp=[img], Tout=tf.string)\n    prompt = tf.expand_dims(prompt, axis=0) \n    prompt.set_shape((1,)) # Necesario para Keras\n    \n    # Estructura: ((Inputs), Target)\n    return (gray, prompt), img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:13:47.511008Z","iopub.execute_input":"2025-12-07T13:13:47.511958Z","iopub.status.idle":"2025-12-07T13:13:47.520664Z","shell.execute_reply.started":"2025-12-07T13:13:47.511930Z","shell.execute_reply":"2025-12-07T13:13:47.520102Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Crear Datasets\ntrain_files = tf.data.Dataset.list_files(os.path.join(train_path, \"*.png\"), seed=SEED)\n# Nota: Usamos take() si el dataset es gigante para probar rápido, quítalo para entrenar completo\ntrain_ds = train_files.take(20000).map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\ntrain_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ntest_files = tf.data.Dataset.list_files(os.path.join(test_path, \"*.png\"), seed=SEED)\ntest_ds = test_files.take(1000).map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\ntest_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:13:48.334633Z","iopub.execute_input":"2025-12-07T13:13:48.335260Z","iopub.status.idle":"2025-12-07T13:13:58.108150Z","shell.execute_reply.started":"2025-12-07T13:13:48.335237Z","shell.execute_reply":"2025-12-07T13:13:58.107534Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, Input\n\ndef residual_block(x, filters, kernel_size=3):\n    res = x\n    x = layers.Conv2D(filters, kernel_size, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(filters, kernel_size, padding='same', activation=None)(x)\n    x = layers.BatchNormalization()(x)\n    if res.shape[-1] != filters:\n        res = layers.Conv2D(filters, 1, padding='same')(res)\n\n    x = layers.Add()([x, res])\n    x = layers.Activation('relu')(x)\n    return x\n\ndef text_guided_AE(input_shape=(96,96,1)):\n    \n    # --- ENTRADAS ---\n    img_input = Input(shape=input_shape, name='image_input')\n    text_input = Input(shape=(1,), dtype=tf.string, name='text_input')\n\n    # --- RAMA DE TEXTO ---\n    # Convertimos texto a vector numérico\n    # Nota: max_tokens=20 porque solo usaremos colores simples (\"red\", \"dark blue\", etc)\n    vectorizer = layers.TextVectorization(max_tokens=20, output_mode='int', output_sequence_length=1)\n    \n    # Embedding: Convierte el índice del token en un vector rico de 128 valores\n    t = vectorizer(text_input)\n    t = layers.Embedding(input_dim=20, output_dim=128)(t)\n    t = layers.Lambda(lambda x: tf.squeeze(x, axis=1), name='squeeze_text_embedding')(t) # Vector de forma (Batch, 128)\n    \n    # Preparamos el texto para inyectarlo en la imagen\n    # Queremos que el texto afecte a toda la imagen espacialmente.\n    # En el bottleneck, la imagen mide 12x12. Repetimos el texto para que coincida.\n    t_repeated = layers.RepeatVector(12 * 12)(t) \n    t_spatial = layers.Reshape((12, 12, 128))(t_repeated) # Forma (Batch, 12, 12, 128)\n\n    # --- RAMA DE IMAGEN (ENCODER) ---\n    # Pre-procesamiento inicial\n    x = layers.Conv2D(64, (3,3), padding='same', activation='relu')(img_input)\n    \n    # Bloque 1\n    x1 = residual_block(x, 64)\n    p1 = layers.MaxPooling2D((2,2))(x1) # Sale 48x48\n\n    # Bloque 2\n    x2 = residual_block(p1, 128)\n    p2 = layers.MaxPooling2D((2,2))(x2) # Sale 24x24\n\n    # Bloque 3\n    x3 = residual_block(p2, 256)\n    p3 = layers.MaxPooling2D((2,2))(x3) # Sale 12x12\n\n    # --- FUSIÓN (BOTTLENECK) ---\n    # Aquí unimos la imagen comprimida (p3) con el texto espacial (t_spatial)\n    # p3 shape: (12, 12, 256)\n    # t_spatial shape: (12, 12, 128)\n    combined = layers.Concatenate()([p3, t_spatial]) # Shape resultante: (12, 12, 384)\n    \n    # Pasamos la fusión por el bloque residual del bottleneck\n    # Esto permite que la red aprenda cómo el texto debe alterar las características visuales\n    b = residual_block(combined, 512)\n\n    # --- DECODER (Simétrico) ---\n    \n    # Subida 3 (De 12x12 a 24x24)\n    u3 = layers.Conv2DTranspose(256, (3,3), strides=(2,2), padding='same', activation='relu')(b)\n    # Concatenamos con x3 (la salida del encoder antes del pooling)\n    c3 = layers.Concatenate()([u3, x3]) \n    d3 = residual_block(c3, 256)\n\n    # Subida 2 (De 24x24 a 48x48)\n    u2 = layers.Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', activation='relu')(d3)\n    c2 = layers.Concatenate()([u2, x2])\n    d2 = residual_block(c2, 128)\n\n    # Subida 1 (De 48x48 a 96x96)\n    u1 = layers.Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', activation='relu')(d2)\n    c1 = layers.Concatenate()([u1, x1])\n    d1 = residual_block(c1, 64)\n\n    # --- OUTPUT ---\n    # Salida RGB (3 canales), Sigmoid para rango [0, 1]\n    outputs = layers.Conv2D(3, (1,1), activation='sigmoid', dtype='float32', name='rgb_output')(d1)\n\n    model = models.Model(inputs=[img_input, text_input], outputs=outputs)\n    return model, vectorizer\n\n# Instanciar \nmodel, vectorizer = text_guided_AE()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:16:40.416313Z","iopub.execute_input":"2025-12-07T13:16:40.416632Z","iopub.status.idle":"2025-12-07T13:16:40.429921Z","shell.execute_reply.started":"2025-12-07T13:16:40.416609Z","shell.execute_reply":"2025-12-07T13:16:40.429211Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(\"Adaptando vocabulario de texto...\")\n\ntext_ds_for_adapt = train_ds.unbatch().map(lambda inputs, target: tf.reshape(inputs[1], [-1])).take(1000)\n# En el map: inputs[1] es (B, 1) -> tf.reshape(..., [-1]) lo convierte a (B,)\n\nvectorizer.adapt(text_ds_for_adapt)\n# ---------------------------------------------\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\nmodel.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n\nprint(\"Vocabulario adaptado con éxito. El modelo está listo para el entrenamiento.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:16:41.716119Z","iopub.execute_input":"2025-12-07T13:16:41.716821Z","iopub.status.idle":"2025-12-07T13:16:49.831955Z","shell.execute_reply.started":"2025-12-07T13:16:41.716791Z","shell.execute_reply":"2025-12-07T13:16:49.831162Z"}},"outputs":[{"name":"stdout","text":"Adaptando vocabulario de texto...\nVocabulario adaptado con éxito. El modelo está listo para el entrenamiento.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(\"Empezando entrenamiento...\")\nhistory = model.fit(\n    train_ds,\n    validation_data=test_ds,\n    epochs=100,\n    verbose=0,\n    callbacks=[TqdmCallback(verbose=1)]\n)\n\nprint(\"Entrenamiento finalizado.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:18:15.278532Z","iopub.execute_input":"2025-12-07T13:18:15.278819Z","iopub.status.idle":"2025-12-07T16:47:31.197583Z","shell.execute_reply.started":"2025-12-07T13:18:15.278797Z","shell.execute_reply":"2025-12-07T16:47:31.196937Z"}},"outputs":[{"name":"stdout","text":"Empezando entrenamiento...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0epoch [00:00, ?epoch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d9dd342d0f44e2598233cbb0e057d88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0batch [00:00, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Entrenamiento finalizado.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# --- CÓDIGO PARA GUARDAR EL MODELO ---\n\nmodel.save('text_guided_colorizer.keras') \nprint(\"Modelo guardado con éxito como 'text_guided_colorizer.keras'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T16:47:31.198776Z","iopub.execute_input":"2025-12-07T16:47:31.199014Z","iopub.status.idle":"2025-12-07T16:47:31.807273Z","shell.execute_reply.started":"2025-12-07T16:47:31.198997Z","shell.execute_reply":"2025-12-07T16:47:31.806424Z"}},"outputs":[{"name":"stdout","text":"Modelo guardado con éxito como 'text_guided_colorizer.keras'\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# --- 1. CONFIGURACIÓN DE PRUEBA ---\n# Importamos el modelo (si lo guardaste y lo cargas en otra sesión)\n# from tensorflow.keras.models import load_model\n# model = load_model('text_guided_colorizer.keras')\n\n# Definimos el prompt de color a probar (¡Esta es la guía del usuario!)\nTEST_PROMPT = \"color blue\" \nTEST_BATCH_SIZE = 32 # Debe coincidir con el BATCH_SIZE usado en el dataset\n\n# --- 2. PREPARAR DATOS DE ENTRADA ---\n\n# Tomar un batch del test_dataset (asumiendo que 'test_ds' está definido y cargado)\n# El test_ds debe tener la estructura: ((gray, text_prompt), color_rgb)\nfor (gray_batch, text_batch), true_color_batch in test_ds.take(1):\n    gray_input = gray_batch        # (BATCH, 96, 96, 1)\n    true_color = true_color_batch  # (BATCH, 96, 96, 3)\n    break\n\n# Creamos un tensor de prompts repetidos para todo el lote de prueba\n# Usamos el prompt de prueba definido por el usuario (TEST_PROMPT)\nuser_prompts = tf.constant([TEST_PROMPT] * TEST_BATCH_SIZE) # Forma (BATCH_SIZE,)\n\n# Necesitamos expandir la dimensión para que coincida con el Input del modelo (B, 1)\n# Si en tu adaptador tenías que el input era (B, 1), esto funciona:\ntext_input_for_model = tf.expand_dims(user_prompts, axis=1) \n# Si tu vectorizer ya maneja (B,), puedes usar user_prompts directamente, \n# pero por seguridad, forzamos la forma (B, 1) para el modelo.\n\n# --- 3. HACER PREDICCIÓN CON DOBLE INPUT ---\n# El modelo espera [Imagen, Texto]\npredicted_colors = model.predict([gray_input, text_input_for_model])\n\n# --- 4. FUNCIÓN DE PLOTEO ---\ndef show_result_guided(i, predictions, gray_data, real_data, prompt):\n    plt.figure(figsize=(12, 4))\n    \n    # Imagen de entrada (grayscale)\n    plt.subplot(1, 3, 1)\n    # Usamos .numpy().squeeze() para eliminar dimensiones extra si existen\n    plt.imshow(gray_data[i].numpy().squeeze(), cmap=\"gray\") \n    plt.title(f\"Input (Gray) | Prompt: {prompt}\")\n    plt.axis(\"off\")\n\n    # Predicción del modelo\n    plt.subplot(1, 3, 2)\n    # Las predicciones ya están en [0, 1] RGB\n    plt.imshow(predictions[i])\n    plt.title(\"Predicted Color\")\n    plt.axis(\"off\")\n\n    # Imagen real (Ground Truth)\n    plt.subplot(1, 3, 3)\n    plt.imshow(real_data[i])\n    plt.title(\"Real Color\")\n    plt.axis(\"off\")\n\n    plt.suptitle(f\"Prueba con Guía de Texto: '{prompt}'\", fontsize=14)\n    plt.show()\n\n# --- 5. MOSTRAR RESULTADOS ---\n\n# Muestra 5 ejemplos del lote, todos coloreados con el mismo prompt\nfor i in range(5):\n    show_result_guided(i, predicted_colors, gray_input, true_color, TEST_PROMPT)\n\nprint(f\"Predicciones mostradas usando el prompt guía: '{TEST_PROMPT}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:12:02.585597Z","iopub.status.idle":"2025-12-07T13:12:02.585862Z","shell.execute_reply.started":"2025-12-07T13:12:02.585734Z","shell.execute_reply":"2025-12-07T13:12:02.585745Z"}},"outputs":[],"execution_count":null}]}