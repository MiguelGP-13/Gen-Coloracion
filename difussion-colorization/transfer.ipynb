{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39470,"sourceType":"datasetVersion","datasetId":30516}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine tuning","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom PIL import Image\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch.optim import AdamW\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, UNet2DModel\n\nfrom datetime import datetime\n\n# Carpeta donde guardar\nSAVE_DIR = \"checkpoints\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\ndef save_model(unet, opt, loss, name=\"unet\"):\n    # Fecha y hora actual\n    timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n    filename = f\"{name}_base_{timestamp}.pt\"\n    path = os.path.join(SAVE_DIR, filename)\n\n    # Guardar estado del modelo y optimizador\n    torch.save({\n        \"model_state_dict\": unet.state_dict(),\n        \"optimizer_state_dict\": opt.state_dict(),\n        \"loss\": loss,\n    }, path)\n    print(f\"Modelo guardado en {path}\")\n\ndef visualize_validation(vae, unet, val_dataset, device=\"cuda\", step_list=[15,30,50], num_samples=2, color_scale=1.0, needs_text=False):\n    indices = np.random.choice(len(val_dataset), num_samples, replace=False)\n    fig, axes = plt.subplots(num_samples, len(step_list)+2, figsize=(15, 5*num_samples), squeeze=False)\n    \n    TEXT_LEN = 77\n    TEXT_DIM = 768\n    \n    for i, idx in enumerate(indices):\n        sample = val_dataset[idx]\n        gray = sample[\"gray\"].unsqueeze(0).to(device)\n        rgb_img = (sample[\"rgb\"].detach().cpu().permute(1,2,0).numpy()*0.5+0.5).clip(0,1)\n        gray_img = (sample[\"gray\"].detach().cpu().permute(1,2,0).numpy()*0.5+0.5).clip(0,1)\n\n        axes[i,0].imshow(gray_img)\n        axes[i,0].set_title(\"Grayscale input\")\n        axes[i,0].axis(\"off\")\n\n        with torch.no_grad():\n            z_gray = vae.encode(gray).latent_dist.sample() * 0.18215\n            \n        \n            if needs_text:\n                text_zeros = torch.zeros((1, TEXT_LEN, TEXT_DIM), device=device)\n\n            for j, steps in enumerate(step_list):\n                z_t = z_gray.clone()\n                ts = torch.linspace(1.0, 0.0, steps, device=device)\n                for t in ts:\n                    t_int = torch.tensor([int(t.item()*999)], device=device)\n                    if needs_text:\n                        delta_t = unet(sample=z_t, timestep=t_int, encoder_hidden_states=text_zeros).sample\n                    else:\n                        delta_t = unet(sample=z_t, timestep=t_int).sample\n                    z_t = z_t + (1.0/steps) * delta_t\n\n                z_col = z_gray + color_scale * (z_t - z_gray)\n                out_rgb = vae.decode(z_col / 0.18215).sample.squeeze(0)\n                out_img = (out_rgb.detach().cpu().permute(1,2,0).numpy()*0.5+0.5).clip(0,1)\n\n                axes[i,j+1].imshow(out_img)\n                axes[i,j+1].set_title(f\"Colorized ({steps} steps)\")\n                axes[i,j+1].axis(\"off\")\n\n        axes[i,len(step_list)+1].imshow(rgb_img)\n        axes[i,len(step_list)+1].set_title(\"Original RGB\")\n        axes[i,len(step_list)+1].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_unet(\n    vae,\n    unet,\n    loader,\n    val_dataset,\n    opt,\n    device=\"cuda\",\n    num_epochs=1000,\n    epochs_visualization=30,\n    needs_text=False\n):\n    # Stable Diffusion CLIP text dims\n    TEXT_LEN = 77\n    TEXT_DIM = 768\n\n    unet.train()\n    vae.eval()\n\n    scaler = torch.cuda.amp.GradScaler(enabled=(device.startswith(\"cuda\") and torch.cuda.is_available()))\n\n    for epoch in range(num_epochs):\n        for batch in loader:\n            z_rgb = batch[\"rgb\"].to(device)\n            z_gray = batch[\"gray\"].to(device)\n\n            # Mezcla temporal\n            t = torch.rand(z_rgb.size(0), device=device)\n            z_t = (1 - t.view(-1,1,1,1)) * z_gray + t.view(-1,1,1,1) * z_rgb\n            target = z_rgb\n\n            # Timesteps enteros\n            t_int = torch.randint(0, 1000, (z_rgb.size(0),), device=device)\n\n            with torch.cuda.amp.autocast(enabled=(device.startswith(\"cuda\") and torch.cuda.is_available())):\n                # Llamada al UNet\n                if needs_text:\n                    text_zeros = torch.zeros((z_rgb.size(0), TEXT_LEN, TEXT_DIM), device=device)\n                    delta_hat = unet(sample=z_t, timestep=t_int, encoder_hidden_states=text_zeros).sample\n                else:\n                    delta_hat = unet(sample=z_t, timestep=t_int).sample\n\n                # Reconstrucción y loss\n                recon = z_t + delta_hat\n                loss = F.mse_loss(recon, target)\n\n            opt.zero_grad(set_to_none=True)\n            scaler.scale(loss).backward()\n            scaler.step(opt)\n            scaler.update()\n\n        # Logs parciales\n        if (epoch + 1) % max(1, (epochs_visualization // 2)) == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}\")\n\n        # Validación cada cierto número de épocas\n        if (epoch + 1) % epochs_visualization == 0:\n            for cs in [1.2, 1.4, 1.6, 1.8]:\n                print(f\"color_scale={cs}\")\n                visualize_validation(\n                    vae=vae,\n                    unet=unet,\n                    val_dataset=val_dataset,\n                    device=device,\n                    step_list=[2, 15, 50, 100],\n                    num_samples=1,\n                    color_scale=cs,\n                    needs_text=needs_text\n                )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:28:45.596938Z","iopub.execute_input":"2025-12-11T11:28:45.597543Z","iopub.status.idle":"2025-12-11T11:28:45.606924Z","shell.execute_reply.started":"2025-12-11T11:28:45.597523Z","shell.execute_reply":"2025-12-11T11:28:45.606284Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"markdown","source":"### Load data","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# Dataset: devuelve pares RGB / Gray\n# -----------------------------\nclass ColorizationDataset(Dataset):\n    def __init__(self, img_dir, size=128):\n        self.img_dir = img_dir\n        self.files = [f for f in os.listdir(img_dir) if f.endswith((\".jpg\",\".png\"))]\n        self.size = size\n        self.to_tensor = transforms.Compose([\n            transforms.Resize((size, size)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5])  # [-1,1]\n        ])\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        path = os.path.join(self.img_dir, self.files[idx])\n        img_rgb = Image.open(path).convert(\"RGB\")\n        img_gray = img_rgb.convert(\"L\").convert(\"RGB\")  # 3 canales para VAE\n\n        rgb_tensor = self.to_tensor(img_rgb)   # [3,H,W]\n        gray_tensor = self.to_tensor(img_gray) # [3,H,W]\n\n        return {\"rgb\": rgb_tensor, \"gray\": gray_tensor}\n\nclass LatentDataset(Dataset):\n    def __init__(self, latent_file):\n        data = torch.load(latent_file)\n        self.rgb = data[\"rgb\"]\n        self.gray = data[\"gray\"]\n\n    def __len__(self):\n        return self.rgb.size(0)\n\n    def __getitem__(self, idx):\n        return {\"rgb\": self.rgb[idx], \"gray\": self.gray[idx]}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:28:45.609822Z","iopub.execute_input":"2025-12-11T11:28:45.610022Z","iopub.status.idle":"2025-12-11T11:28:45.629023Z","shell.execute_reply.started":"2025-12-11T11:28:45.610006Z","shell.execute_reply":"2025-12-11T11:28:45.628274Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Dataset original con imágenes\n# Limitar a un máximo de imágenes\ndef generar_latente(max_images = 100, path = None):\n    dataset = ColorizationDataset(\"/kaggle/input/stl10/unlabeled_images\", size=128)\n    if len(dataset) > max_images:\n        dataset.files = dataset.files[:max_images]\n    \n    loader = DataLoader(dataset, batch_size=16, shuffle=False)\n    \n    \n    vae = AutoencoderKL.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        subfolder=\"vae\"\n    ).to(\"cuda\")\n    vae.requires_grad_(False)\n    \n    latents_rgb, latents_gray = [], []\n    \n    with torch.no_grad():\n        for batch in loader:\n            rgb = batch[\"rgb\"].to(\"cuda\")\n            gray = batch[\"gray\"].to(\"cuda\")\n    \n            z_rgb = vae.encode(rgb).latent_dist.sample() * 0.18215\n            z_gray = vae.encode(gray).latent_dist.sample() * 0.18215\n    \n            latents_rgb.append(z_rgb.cpu())\n            latents_gray.append(z_gray.cpu())\n    \n    latents_rgb = torch.cat(latents_rgb)\n    latents_gray = torch.cat(latents_gray)\n    if not path:\n        path = f\"stl10_latents_{max_images}.pt\"\n    torch.save({\"rgb\": latents_rgb, \"gray\": latents_gray}, path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:28:47.383067Z","iopub.execute_input":"2025-12-11T11:28:47.383784Z","iopub.status.idle":"2025-12-11T11:28:47.390007Z","shell.execute_reply.started":"2025-12-11T11:28:47.383756Z","shell.execute_reply":"2025-12-11T11:28:47.389408Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Cargar hiperparámetros","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# Hiperparámetros\n# -----------------------------\nNUM_EPOCHS = 10000\nEPOCHS_VISUALIZATION = 20\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\nLR = 1e-3\nSIZE = 128\n\n# -----------------------------\n# Modelos: VAE congelado y UNet entrenable\n# -----------------------------\ndevice = DEVICE\n\nvae = AutoencoderKL.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        subfolder=\"vae\"\n    ).to(\"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:28:48.943172Z","iopub.execute_input":"2025-12-11T11:28:48.943439Z","iopub.status.idle":"2025-12-11T11:28:49.367111Z","shell.execute_reply.started":"2025-12-11T11:28:48.943422Z","shell.execute_reply":"2025-12-11T11:28:49.366298Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Cargamos modelo y datos","metadata":{}},{"cell_type":"code","source":"stableDifussion = UNet2DConditionModel.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",  # o el checkpoint que quieras\n    subfolder=\"unet\"\n).to(DEVICE)\nfor name, param in stableDifussion.named_parameters():\n    if \"up_blocks.2\" in name or \"mid_block\" in name:\n        param.requires_grad = True   # entrenar últimas capas\n    else:\n        param.requires_grad = False  # congelar resto\n\n\n# -----------------------------\n# Dataloader\n# -----------------------------\ndataset = LatentDataset(\"stl10_latents_25000.pt\")\n\n# Dataset de validación (aún con imágenes, para visualizar resultados)\nval_dataset = ColorizationDataset(\"/kaggle/input/stl10/train_images\", size=SIZE)\n\nloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n\n# -----------------------------\n# Optimizador\n# -----------------------------\nopt = AdamW(stableDifussion.parameters(), lr=1e-5, weight_decay=0.01)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:28:51.848210Z","iopub.execute_input":"2025-12-11T11:28:51.848950Z","iopub.status.idle":"2025-12-11T11:28:53.434627Z","shell.execute_reply.started":"2025-12-11T11:28:51.848920Z","shell.execute_reply":"2025-12-11T11:28:53.433742Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"EPOCHS_VISUALIZATION = 30\ntrain_unet(vae, stableDifussion, loader, val_dataset, opt, device=DEVICE, num_epochs=1000, needs_text=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:28:53.435763Z","iopub.execute_input":"2025-12-11T11:28:53.436057Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_233/1099172403.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.startswith(\"cuda\") and torch.cuda.is_available()))\n/tmp/ipykernel_233/1099172403.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.startswith(\"cuda\") and torch.cuda.is_available())):\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Guardado","metadata":{}},{"cell_type":"code","source":"visualize_validation(vae, stableDifussion, val_dataset, device=DEVICE, step_list=[2,15,50,100], num_samples=1, color_scale=1.5)\nsave_model(stableDifussion, opt, loss, name=\"stableDifussion\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inferencia","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"class Colorizer:\n    def __init__(self, vae: AutoencoderKL, unet: UNet2DConditionModel, device='cuda'):\n        self.vae = vae.eval().to(device)\n        self.unet = unet.eval().to(device)\n        self.device = device\n\n    def _preprocess_gray(self, img_gray_pil, size=512):\n        img = img_gray_pil.resize((size, size), Image.BICUBIC)\n        arr = np.array(img.convert(\"L\").convert(\"RGB\")).astype(np.float32) / 255.0\n        arr = (arr - 0.5) / 0.5\n        return torch.from_numpy(arr).permute(2,0,1).unsqueeze(0).to(self.device)  # [1,3,H,W]\n\n    def _replace_luma(self, out_rgb_norm, in_gray_norm):\n        out = out_rgb_norm.permute(1,2,0).cpu().numpy()*0.5+0.5\n        gray = in_gray_norm[0].cpu().numpy()*0.5+0.5\n        out_bgr = cv2.cvtColor((out*255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n        lab = cv2.cvtColor(out_bgr, cv2.COLOR_BGR2Lab)\n        lab[:,:,0] = (gray*255).astype(np.uint8)\n        bgr = cv2.cvtColor(lab, cv2.COLOR_Lab2BGR)\n        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n        return Image.fromarray(rgb)\n\n    @torch.no_grad()\n    def colorize(self, gray_img_pil: Image.Image, steps=50, size=512, color_scale=1.0):\n        # 1) Preprocesar y codificar\n        gray = self._preprocess_gray(gray_img_pil, size)  # [1,3,H,W]\n        z_gray = self.vae.encode(gray).latent_dist.sample() * 0.18215\n        z_t = z_gray.clone()\n\n        # 2) Iteración de refinamiento\n        ts = torch.linspace(1.0, 0.0, steps, device=self.device)\n        for t in ts:\n            t_int = torch.tensor([int(t.item()*999)], device=self.device)\n            text_emb = torch.zeros((1,77,768), device=self.device)  # dummy text conditioning\n            delta_t = self.unet(sample=z_t, timestep=t_int, encoder_hidden_states=text_emb).sample\n            z_t = z_t + (1.0/steps) * delta_t\n\n        # 3) Escalado y decodificación\n        z_col = z_gray + color_scale * (z_t - z_gray)\n        out_rgb = self.vae.decode(z_col / 0.18215).sample.squeeze(0)\n\n        # 4) Reemplazo de luminancia\n        final_img = self._replace_luma(out_rgb, gray.squeeze(0))\n        return final_img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:28:38.620550Z","iopub.status.idle":"2025-12-11T11:28:38.621076Z","shell.execute_reply.started":"2025-12-11T11:28:38.620929Z","shell.execute_reply":"2025-12-11T11:28:38.620945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lpips  # pip install lpips\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# --- Cargar VAE y UNet entrenado ---\nvae = AutoencoderKL.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    subfolder=\"vae\"\n).to(DEVICE)\nvae.requires_grad_(False)\n\nunet = UNet2DModel(\n    sample_size=16,  \n    in_channels=4,\n    out_channels=4,\n    layers_per_block=2,\n    block_out_channels=(128, 256, 256),\n    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"),\n    up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\")\n).to(DEVICE)\n\ncheckpoint = torch.load(\"checkpoints/unet_epoch500_loss0.0123_20251209_1855.pt\", map_location=DEVICE)\nunet.load_state_dict(checkpoint[\"model_state_dict\"])\nunet.eval()\n\ncolorizer = Colorizer(vae, unet, device=DEVICE)\n\n# --- Transformaciones ---\ntransform = transforms.Compose([\n    transforms.Resize((128,128)),\n    transforms.ToTensor()\n])\n\n# --- Métricas ---\nlpips_fn = lpips.LPIPS(net='alex').to(DEVICE)\n\ndef evaluate_folder(folder_path, num_samples=100):\n    psnr_scores, ssim_scores, lpips_scores = [], [], []\n    files = os.listdir(folder_path)[:num_samples]\n\n    for fname in files:\n        img = Image.open(os.path.join(folder_path, fname)).convert(\"RGB\")\n        gray = img.convert(\"L\")\n\n        # Colorizar\n        colorized = colorizer.colorize(gray, steps=50, size=256, color_scale=1.0)\n\n        # Convertir a tensores\n        img_t = transform(img).unsqueeze(0).to(DEVICE)\n        col_t = transform(colorized).unsqueeze(0).to(DEVICE)\n\n        # LPIPS\n        lp = lpips_fn(img_t, col_t).item()\n        lpips_scores.append(lp)\n\n        # PSNR y SSIM (en CPU con numpy)\n        img_np = np.array(img.resize((256,256))).astype(np.float32)\n        col_np = np.array(colorized.resize((256,256))).astype(np.float32)\n\n        psnr_scores.append(peak_signal_noise_ratio(img_np, col_np, data_range=255))\n        ssim_scores.append(structural_similarity(img_np, col_np, channel_axis=2))\n\n    print(f\"LPIPS: {np.mean(lpips_scores):.4f}\")\n    print(f\"PSNR: {np.mean(psnr_scores):.2f}\")\n    print(f\"SSIM: {np.mean(ssim_scores):.4f}\")\n\n# --- Ejecutar evaluación ---\nevaluate_folder(\"/kaggle/input/stl10/test_images\", num_samples=200)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:28:38.621788Z","iopub.status.idle":"2025-12-11T11:28:38.622094Z","shell.execute_reply.started":"2025-12-11T11:28:38.621912Z","shell.execute_reply":"2025-12-11T11:28:38.621927Z"}},"outputs":[],"execution_count":null}]}