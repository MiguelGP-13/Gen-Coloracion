{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":39470,"sourceType":"datasetVersion","datasetId":30516}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine tuning","metadata":{}},{"cell_type":"code","source":"# !pip install lpips\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom PIL import Image\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch.optim import AdamW\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, UNet2DModel\n\nfrom datetime import datetime\n\nimport lpips\nfrom skimage.metrics import structural_similarity\nfrom skimage.color import rgb2lab, deltaE_ciede2000\n\n# Carpeta donde guardar\nSAVE_DIR = \"checkpoints\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\ndef save_model(unet, opt, name=\"unet\"):\n    # Fecha y hora actual\n    timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n    filename = f\"{name}_base_{timestamp}.pt\"\n    path = os.path.join(SAVE_DIR, filename)\n\n    # Guardar estado del modelo y optimizador\n    torch.save({\n        \"model_state_dict\": unet.state_dict(),\n        \"optimizer_state_dict\": opt.state_dict(),\n    }, path)\n    print(f\"Modelo guardado en {path}\")\n\n# -----------------------------\n# Cargar modelo y optimizador\n# -----------------------------\ndef load_model(unet, opt, checkpoint_path):\n    # Cargar el diccionario guardado\n    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n\n    # Restaurar pesos del modelo\n    unet.load_state_dict(checkpoint[\"model_state_dict\"])\n\n    # Restaurar estado del optimizador\n    opt.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n    print(f\"Modelo y optimizador cargados desde {checkpoint_path}\")\n    return unet, opt\n\n\ndef visualize_validation(\n    vae,\n    unet,\n    val_dataset,\n    unet_device=\"cuda\",\n    vae_device=\"cpu\",\n    step_list=[15,30,50],\n    num_samples=2,\n    color_scale=1.0,\n    needs_text=False\n):\n    indices = np.random.choice(len(val_dataset), num_samples, replace=False)\n    fig, axes = plt.subplots(num_samples, len(step_list)+2, figsize=(15, 5*num_samples), squeeze=False)\n    \n    TEXT_LEN = 77\n    TEXT_DIM = 768\n    \n    for i, idx in enumerate(indices):\n        sample = val_dataset[idx]\n        # Entrada al VAE en vae_device\n        gray = sample[\"gray\"].unsqueeze(0).to(vae_device)\n        rgb_img = (sample[\"rgb\"].detach().cpu().permute(1,2,0).numpy()*0.5+0.5).clip(0,1)\n        gray_img = (sample[\"gray\"].detach().cpu().permute(1,2,0).numpy()*0.5+0.5).clip(0,1)\n\n        axes[i,0].imshow(gray_img)\n        axes[i,0].set_title(\"Grayscale input\")\n        axes[i,0].axis(\"off\")\n\n        with torch.no_grad():\n            # Codificación con VAE en vae_device\n            z_gray_cpu = vae.encode(gray).latent_dist.sample() * 0.18215\n            # Mover latente al dispositivo del UNet\n            z_gray = z_gray_cpu.to(unet_device)\n            \n            if needs_text:\n                text_zeros = torch.zeros((1, TEXT_LEN, TEXT_DIM), device=unet_device)\n\n            for j, steps in enumerate(step_list):\n                z_t = z_gray.clone()\n                ts = torch.linspace(1.0, 0.0, steps, device=unet_device)\n                for t in ts:\n                    t_int = torch.tensor([int(t.item()*999)], device=unet_device)\n                    if needs_text:\n                        delta_t = unet(sample=z_t, timestep=t_int, encoder_hidden_states=text_zeros).sample\n                    else:\n                        delta_t = unet(sample=z_t, timestep=t_int).sample\n                    z_t = z_t + (1.0/steps) * delta_t\n\n                # Volver al vae_device para decodificar\n                z_col_cpu = (z_gray + color_scale * (z_t - z_gray)).to(vae_device)\n                out_rgb = vae.decode(z_col_cpu / 0.18215).sample.squeeze(0)\n                out_img = (out_rgb.detach().cpu().permute(1,2,0).numpy()*0.5+0.5).clip(0,1)\n\n                axes[i,j+1].imshow(out_img)\n                axes[i,j+1].set_title(f\"Colorized ({steps} steps)\")\n                axes[i,j+1].axis(\"off\")\n\n        axes[i,len(step_list)+1].imshow(rgb_img)\n        axes[i,len(step_list)+1].set_title(\"Original RGB\")\n        axes[i,len(step_list)+1].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:13:09.235063Z","iopub.execute_input":"2025-12-12T14:13:09.235304Z","iopub.status.idle":"2025-12-12T14:13:20.518049Z","shell.execute_reply.started":"2025-12-12T14:13:09.235278Z","shell.execute_reply":"2025-12-12T14:13:20.517337Z"}},"outputs":[{"name":"stderr","text":"2025-12-12 14:13:15.390232: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765548795.411122     490 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765548795.417466     490 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"def train_unet(\n    vae,\n    unet,\n    loader,\n    val_dataset,\n    opt,\n    device=\"cuda\",\n    num_epochs=1000,\n    epochs_visualization=30,\n    needs_text=False\n):\n    # Stable Diffusion CLIP text dims\n    TEXT_LEN = 77\n    TEXT_DIM = 768\n\n    unet.train()\n    vae.eval()\n\n    scaler = torch.cuda.amp.GradScaler(enabled=(device.startswith(\"cuda\") and torch.cuda.is_available()))\n\n    for epoch in range(num_epochs):\n        for batch in loader:\n            z_rgb = batch[\"rgb\"].to(device)\n            z_gray = batch[\"gray\"].to(device)\n\n            # Mezcla temporal\n            t = torch.rand(z_rgb.size(0), device=device)\n            z_t = (1 - t.view(-1,1,1,1)) * z_gray + t.view(-1,1,1,1) * z_rgb\n            target = z_rgb\n\n            # Timesteps enteros\n            t_int = torch.randint(0, 1000, (z_rgb.size(0),), device=device)\n\n            with torch.cuda.amp.autocast(enabled=(device.startswith(\"cuda\") and torch.cuda.is_available())):\n                # Llamada al UNet\n                if needs_text:\n                    text_zeros = torch.zeros((z_rgb.size(0), TEXT_LEN, TEXT_DIM), device=device)\n                    delta_hat = unet(sample=z_t, timestep=t_int, encoder_hidden_states=text_zeros).sample\n                else:\n                    delta_hat = unet(sample=z_t, timestep=t_int).sample\n\n                # Reconstrucción y loss\n                recon = z_t + delta_hat\n                loss = F.mse_loss(recon, target)\n\n            opt.zero_grad(set_to_none=True)\n            scaler.scale(loss).backward()\n            scaler.step(opt)\n            scaler.update()\n\n        # Logs parciales\n        if (epoch + 1) % max(1, (epochs_visualization // 2)) == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}\")\n\n        # Validación cada cierto número de épocas\n        if (epoch + 1) % epochs_visualization == 0:\n            for cs in [1.2, 1.4, 1.6, 1.8]:\n                print(f\"color_scale={cs}\")\n                visualize_validation(\n                    vae=vae,\n                    unet=unet,\n                    val_dataset=val_dataset,\n                    unet_device=device, vae_device=\"cpu\",\n                    step_list=[2, 15, 50, 100],\n                    num_samples=1,\n                    color_scale=cs,\n                    needs_text=needs_text\n                )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:08:51.300230Z","iopub.execute_input":"2025-12-12T14:08:51.300948Z","iopub.status.idle":"2025-12-12T14:08:51.310063Z","shell.execute_reply.started":"2025-12-12T14:08:51.300921Z","shell.execute_reply":"2025-12-12T14:08:51.309422Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"markdown","source":"### Load data","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# Dataset: devuelve pares RGB / Gray\n# -----------------------------\nclass ColorizationDataset(Dataset):\n    def __init__(self, img_dir, size=128):\n        self.img_dir = img_dir\n        self.files = [f for f in os.listdir(img_dir) if f.endswith((\".jpg\",\".png\"))]\n        self.size = size\n        self.to_tensor = transforms.Compose([\n            transforms.Resize((size, size)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5])  # [-1,1]\n        ])\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        path = os.path.join(self.img_dir, self.files[idx])\n        img_rgb = Image.open(path).convert(\"RGB\")\n        img_gray = img_rgb.convert(\"L\").convert(\"RGB\")  # 3 canales para VAE\n\n        rgb_tensor = self.to_tensor(img_rgb)   # [3,H,W]\n        gray_tensor = self.to_tensor(img_gray) # [3,H,W]\n\n        return {\"rgb\": rgb_tensor, \"gray\": gray_tensor}\n\nclass LatentDataset(Dataset):\n    def __init__(self, latent_file):\n        data = torch.load(latent_file)\n        self.rgb = data[\"rgb\"]\n        self.gray = data[\"gray\"]\n\n    def __len__(self):\n        return self.rgb.size(0)\n\n    def __getitem__(self, idx):\n        return {\"rgb\": self.rgb[idx], \"gray\": self.gray[idx]}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:07:57.211335Z","iopub.status.idle":"2025-12-12T14:07:57.211601Z","shell.execute_reply.started":"2025-12-12T14:07:57.211494Z","shell.execute_reply":"2025-12-12T14:07:57.211505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset original con imágenes\n# Limitar a un máximo de imágenes\ndef generar_latente(max_images = 100, path = None):\n    dataset = ColorizationDataset(\"/kaggle/input/stl10/unlabeled_images\", size=128)\n    if len(dataset) > max_images:\n        dataset.files = dataset.files[:max_images]\n    \n    loader = DataLoader(dataset, batch_size=16, shuffle=False)\n    \n    \n    vae = AutoencoderKL.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        subfolder=\"vae\"\n    ).to(\"cuda\")\n    vae.requires_grad_(False)\n    \n    latents_rgb, latents_gray = [], []\n    \n    with torch.no_grad():\n        for batch in loader:\n            rgb = batch[\"rgb\"].to(\"cuda\")\n            gray = batch[\"gray\"].to(\"cuda\")\n    \n            z_rgb = vae.encode(rgb).latent_dist.sample() * 0.18215\n            z_gray = vae.encode(gray).latent_dist.sample() * 0.18215\n    \n            latents_rgb.append(z_rgb.cpu())\n            latents_gray.append(z_gray.cpu())\n    \n    latents_rgb = torch.cat(latents_rgb)\n    latents_gray = torch.cat(latents_gray)\n    if not path:\n        path = f\"stl10_latents_{max_images}.pt\"\n    torch.save({\"rgb\": latents_rgb, \"gray\": latents_gray}, path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:07:57.213043Z","iopub.status.idle":"2025-12-12T14:07:57.213349Z","shell.execute_reply.started":"2025-12-12T14:07:57.213183Z","shell.execute_reply":"2025-12-12T14:07:57.213197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cargar hiperparámetros","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# Hiperparámetros\n# -----------------------------\nNUM_EPOCHS = 200\nEPOCHS_VISUALIZATION = 20\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\nLR = 1e-5\nSIZE = 128\n\n# -----------------------------\n# Modelos: VAE congelado y UNet entrenable\n# -----------------------------\ndevice = DEVICE\n\nvae = AutoencoderKL.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        subfolder=\"vae\"\n    ).to(\"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:07:57.214558Z","iopub.status.idle":"2025-12-12T14:07:57.214824Z","shell.execute_reply.started":"2025-12-12T14:07:57.214691Z","shell.execute_reply":"2025-12-12T14:07:57.214701Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cargamos modelo y datos","metadata":{}},{"cell_type":"code","source":"stableDifussion = UNet2DConditionModel.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",  # o el checkpoint que quieras\n    subfolder=\"unet\"\n).to(DEVICE)\nfor name, param in stableDifussion.named_parameters():\n    if \"up_blocks.2\" in name or \"mid_block\" in name:\n        param.requires_grad = True   # entrenar últimas capas\n    else:\n        param.requires_grad = False  # congelar resto\n\n\n# -----------------------------\n# Dataloader\n# -----------------------------\ndataset = LatentDataset(\"/kaggle/working/stl10_latents_10000.pt\")\n\n# Dataset de validación (aún con imágenes, para visualizar resultados)\nval_dataset = ColorizationDataset(\"/kaggle/input/stl10/train_images\", size=SIZE)\n\nloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n\n# -----------------------------\n# Optimizador\n# -----------------------------\nopt = AdamW(stableDifussion.parameters(), lr=LR, weight_decay=0.01)\n\n# 2. Cargar el checkpoint\ncheckpoint_path = \"/kaggle/working/stableDifussion_base_1212_1047.pt\"  # tu archivo guardado\nstableDifussion, opt = load_model(stableDifussion, opt, checkpoint_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:07:57.215602Z","iopub.status.idle":"2025-12-12T14:07:57.215864Z","shell.execute_reply.started":"2025-12-12T14:07:57.215722Z","shell.execute_reply":"2025-12-12T14:07:57.215736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS_VISUALIZATION = 4\ntrain_unet(vae, stableDifussion, loader, val_dataset, opt, device=DEVICE, num_epochs=NUM_EPOCHS, needs_text=True, epochs_visualization=EPOCHS_VISUALIZATION)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:07:57.217102Z","iopub.status.idle":"2025-12-12T14:07:57.217442Z","shell.execute_reply.started":"2025-12-12T14:07:57.217255Z","shell.execute_reply":"2025-12-12T14:07:57.217269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Guardado","metadata":{}},{"cell_type":"code","source":"visualize_validation(vae, stableDifussion, val_dataset, step_list=[2,100, 200, 500], num_samples=1, color_scale=1.5, needs_text=True)\nsave_model(stableDifussion, opt, name=\"stableDifussion\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:07:57.218591Z","iopub.status.idle":"2025-12-12T14:07:57.218837Z","shell.execute_reply.started":"2025-12-12T14:07:57.218727Z","shell.execute_reply":"2025-12-12T14:07:57.218738Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inferencia","metadata":{}},{"cell_type":"code","source":"class Colorizer:\n    def __init__(self, vae: AutoencoderKL, unet: UNet2DConditionModel, device='cuda'):\n        self.vae = vae.eval().to(device)\n        self.unet = unet.eval().to(device)\n        self.device = device\n\n    def _preprocess_gray(self, img_gray_pil, size=512):\n        img = img_gray_pil.resize((size, size), Image.BICUBIC)\n        arr = np.array(img.convert(\"L\").convert(\"RGB\")).astype(np.float32) / 255.0\n        arr = (arr - 0.5) / 0.5\n        return torch.from_numpy(arr).permute(2,0,1).unsqueeze(0).to(self.device)  # [1,3,H,W]\n\n    def _replace_luma(self, out_rgb_norm, in_gray_norm):\n        out = out_rgb_norm.permute(1,2,0).cpu().numpy()*0.5+0.5\n        gray = in_gray_norm[0].cpu().numpy()*0.5+0.5\n        out_bgr = cv2.cvtColor((out*255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n        lab = cv2.cvtColor(out_bgr, cv2.COLOR_BGR2Lab)\n        lab[:,:,0] = (gray*255).astype(np.uint8)\n        bgr = cv2.cvtColor(lab, cv2.COLOR_Lab2BGR)\n        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n        return Image.fromarray(rgb)\n\n    @torch.no_grad()\n    def colorize(self, gray_img_pil: Image.Image, steps=50, size=512, color_scale=1.0):\n        # 1) Preprocesar y codificar\n        gray = self._preprocess_gray(gray_img_pil, size)  # [1,3,H,W]\n        z_gray = self.vae.encode(gray).latent_dist.sample() * 0.18215\n        z_t = z_gray.clone()\n\n        # 2) Iteración de refinamiento\n        ts = torch.linspace(1.0, 0.0, steps, device=self.device)\n        for t in ts:\n            t_int = torch.tensor([int(t.item()*999)], device=self.device)\n            text_emb = torch.zeros((1,77,768), device=self.device)  # dummy text conditioning\n            delta_t = self.unet(sample=z_t, timestep=t_int, encoder_hidden_states=text_emb).sample\n            z_t = z_t + (1.0/steps) * delta_t\n\n        # 3) Escalado y decodificación\n        z_col = z_gray + color_scale * (z_t - z_gray)\n        out_rgb = self.vae.decode(z_col / 0.18215).sample.squeeze(0)\n\n        # 4) Reemplazo de luminancia\n        final_img = self._replace_luma(out_rgb, gray.squeeze(0))\n        return final_img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:13:20.521916Z","iopub.execute_input":"2025-12-12T14:13:20.522083Z","iopub.status.idle":"2025-12-12T14:13:20.531185Z","shell.execute_reply.started":"2025-12-12T14:13:20.522068Z","shell.execute_reply":"2025-12-12T14:13:20.530637Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nos.listdir(\"/kaggle/working/checkpoints\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:13:20.531794Z","iopub.execute_input":"2025-12-12T14:13:20.531986Z","iopub.status.idle":"2025-12-12T14:13:20.548632Z","shell.execute_reply.started":"2025-12-12T14:13:20.531970Z","shell.execute_reply":"2025-12-12T14:13:20.547952Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['unet_base_1210_1625.pt', 'stableDifussion_base_1212_1328.pt']"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# --- Cargar VAE y UNet entrenado ---\nvae = AutoencoderKL.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    subfolder=\"vae\"\n).to(\"cpu\")\nvae.requires_grad_(False)\n\nstableDifussion = UNet2DConditionModel.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",  # o el checkpoint que quieras\n    subfolder=\"unet\"\n).to(DEVICE)\nopt = AdamW(stableDifussion.parameters(), lr=1e-5, weight_decay=0.01)\n\ncheckpoint_path = \"/kaggle/working/checkpoints/stableDifussion_base_1212_1328.pt\"  # tu archivo guardado\nstableDifussion, opt = load_model(stableDifussion, opt, checkpoint_path)\n\ncolorizer = Colorizer(vae, stableDifussion, device=DEVICE)\n\n# --- Transformaciones ---\ntransform = transforms.Compose([\n    transforms.Resize((128,128)),\n    transforms.ToTensor()\n])\n\n# --- Métricas ---\nlpips_fn = lpips.LPIPS(net='alex').to(DEVICE)\n\ndef evaluate_folder(folder_path, num_samples=100):\n    mse_scores, ssim_scores, lpips_scores, ciede_scores = [], [], [], []\n    files = os.listdir(folder_path)[:num_samples]\n\n    for fname in files:\n        img = Image.open(os.path.join(folder_path, fname)).convert(\"RGB\")\n        gray = img.convert(\"L\")\n\n        # Colorizar (usa UNet en GPU y VAE en CPU)\n        colorized = colorizer.colorize(gray, steps=10, size=256, color_scale=1.5)\n\n        # Tensores para LPIPS (en GPU)\n        img_t = transform(img).unsqueeze(0).to(DEVICE)\n        col_t = transform(colorized).unsqueeze(0).to(DEVICE)\n\n        with torch.no_grad():\n            lp = lpips_fn(img_t, col_t).item()\n        lpips_scores.append(lp)\n\n        # Convertir a numpy para métricas en CPU\n        img_np = np.array(img.resize((256,256))).astype(np.float32)\n        col_np = np.array(colorized.resize((256,256))).astype(np.float32)\n\n        # --- MSE ---\n        mse = np.mean((img_np - col_np) ** 2)\n        mse_scores.append(mse)\n\n        # --- SSIM ---\n        ssim_scores.append(structural_similarity(img_np, col_np, channel_axis=2, data_range=255))\n\n        # --- CIEDE2000 ---\n        img_lab = rgb2lab(img_np / 255.0)\n        col_lab = rgb2lab(col_np / 255.0)\n        delta_e = deltaE_ciede2000(img_lab, col_lab)\n        ciede_scores.append(np.mean(delta_e))\n\n        # Liberar memoria GPU en cada iteración\n        del img_t, col_t\n        torch.cuda.empty_cache()\n\n    print(f\"LPIPS: {np.mean(lpips_scores):.4f}\")\n    print(f\"MSE: {np.mean(mse_scores):.2f}\")\n    print(f\"SSIM: {np.mean(ssim_scores):.4f}\")\n    print(f\"CIEDE2000: {np.mean(ciede_scores):.2f}\")\n\n\n\n# --- Ejecutar evaluación ---\nevaluate_folder(\"/kaggle/input/stl10/test_images\", num_samples=4000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T15:39:08.871959Z","iopub.execute_input":"2025-12-12T15:39:08.872244Z","iopub.status.idle":"2025-12-12T16:30:40.250405Z","shell.execute_reply.started":"2025-12-12T15:39:08.872222Z","shell.execute_reply":"2025-12-12T16:30:40.249735Z"}},"outputs":[{"name":"stdout","text":"Modelo y optimizador cargados desde /kaggle/working/checkpoints/stableDifussion_base_1212_1328.pt\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.11/dist-packages/lpips/weights/v0.1/alex.pth\nLPIPS: 0.1416\nMSE: 554.04\nSSIM: 0.8636\nCIEDE2000: 13.76\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import shutil\n#shutil.copy(\"/kaggle/working/checkpoints/stableDifussion_base_1212_1328.pt\",\"stableDifussion_base_1212_1328.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T17:09:22.612759Z","iopub.execute_input":"2025-12-12T17:09:22.613283Z","iopub.status.idle":"2025-12-12T17:09:25.488110Z","shell.execute_reply.started":"2025-12-12T17:09:22.613255Z","shell.execute_reply":"2025-12-12T17:09:25.487324Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'stableDifussion_base_1212_1328.pt'"},"metadata":{}}],"execution_count":8}]}